{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "unique-allah",
      "metadata": {
        "id": "unique-allah"
      },
      "source": [
        "# Workshop #3: Multivariable Calculus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "korean-thanksgiving",
      "metadata": {
        "id": "korean-thanksgiving"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "import scipy.optimize as opt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "essential-impression",
      "metadata": {
        "id": "essential-impression"
      },
      "source": [
        "## Problem 1\n",
        "Find the critical points of the function $f(x, y) = x^2 + y^4 - 4xy$ by using first partial derivatives. Then use second partial derivatives to establish whether each critical point is a local minimum, a local maximum, or a saddle point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "powerful-montgomery",
      "metadata": {
        "id": "powerful-montgomery",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0888fd7-eb06-4d59-fa5a-b4291d66b764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Critical points for f: [(0, 0), (-2*sqrt(2), -sqrt(2)), (2*sqrt(2), sqrt(2))]\n",
            "\n",
            "At point (0, 0):\n",
            "D = -16\n",
            "Saddle Point\n",
            "\n",
            "At point (-2*sqrt(2), -sqrt(2)):\n",
            "D = 32\n",
            "Local Minimum\n",
            "\n",
            "At point (2*sqrt(2), sqrt(2)):\n",
            "D = 32\n",
            "Local Minimum\n"
          ]
        }
      ],
      "source": [
        "# Define the variables and the function\n",
        "x, y = sp.symbols('x y')\n",
        "f = x**2 + y**4 - 4*x*y\n",
        "\n",
        "# Differentiate\n",
        "f_x = sp.diff(f, x)\n",
        "f_y = sp.diff(f, y)\n",
        "\n",
        "# Find critical points\n",
        "critical_points = sp.solve([f_x, f_y], (x, y))\n",
        "print(\"Critical points for f:\", critical_points)\n",
        "\n",
        "# Classifying the critical points\n",
        "# Getting the second derivatives\n",
        "f_xx = sp.diff(f_x, x)\n",
        "f_yy = sp.diff(f_y, y)\n",
        "f_xy = sp.diff(f_x, y)\n",
        "\n",
        "# Working with the first critical point\n",
        "point_1 = critical_points[0]\n",
        "p_1 = f_xx.subs({x: point_1[0], y: point_1[1]})\n",
        "p_2 = f_yy.subs({x: point_1[0], y: point_1[1]})\n",
        "p_3 = f_xy.subs({x: point_1[0], y: point_1[1]})\n",
        "\n",
        "D1 = p_1 * p_2 - p_3**2\n",
        "print()\n",
        "print(f\"At point {point_1}:\")\n",
        "print(f\"D = {D1}\")\n",
        "if D1 > 0:\n",
        "  if p_1 > 0:\n",
        "    print(\"Local Minimum\")\n",
        "  elif p_1 < 0:\n",
        "    print(\"Local Maximum\")\n",
        "elif D1 < 0:\n",
        "  print(\"Saddle Point\")\n",
        "else:\n",
        "  print(\"Test Inconclusive\")\n",
        "\n",
        "# Working with the second critical point\n",
        "point_2 = critical_points[1]\n",
        "p_1 = f_xx.subs({x: point_2[0], y: point_2[1]})\n",
        "p_2 = f_yy.subs({x: point_2[0], y: point_2[1]})\n",
        "p_3 = f_xy.subs({x: point_2[0], y: point_2[1]})\n",
        "\n",
        "D2 = p_1 * p_2 - p_3**2\n",
        "print()\n",
        "print(f\"At point {point_2}:\")\n",
        "print(f\"D = {D2}\")\n",
        "if D2 > 0:\n",
        "  if p_1 > 0:\n",
        "    print(\"Local Minimum\")\n",
        "  elif p_1 < 0:\n",
        "    print(\"Local Maximum\")\n",
        "elif D2 < 0:\n",
        "  print(\"Saddle Point\")\n",
        "else:\n",
        "  print(\"Test Inconclusive\")\n",
        "\n",
        "# Working with the third critical point\n",
        "point_3 = critical_points[2]\n",
        "p_1 = f_xx.subs({x: point_3[0], y: point_3[1]})\n",
        "p_2 = f_yy.subs({x: point_3[0], y: point_3[1]})\n",
        "p_3 = f_xy.subs({x: point_3[0], y: point_3[1]})\n",
        "\n",
        "D3 = p_1 * p_2 - p_3**2\n",
        "print()\n",
        "print(f\"At point {point_3}:\")\n",
        "print(f\"D = {D3}\")\n",
        "if D3 > 0:\n",
        "  if p_1 > 0:\n",
        "    print(\"Local Minimum\")\n",
        "  elif p_1 < 0:\n",
        "    print(\"Local Maximum\")\n",
        "elif D3 < 0:\n",
        "  print(\"Saddle Point\")\n",
        "else:\n",
        "  print(\"Test Inconclusive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-newman",
      "metadata": {
        "id": "linear-newman"
      },
      "source": [
        "## Problem 2\n",
        "Using the **Gradient Descent Method** with initial approximation $\\mathbf{x}_0 = (0, 0)$, find the minimum point and the minimum value of the function $g(x, y) = (1 - x + x^2)\\cdot e^{y^2} + (1 - y + y^2)\\cdot e^{x^2}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certain-wrestling",
      "metadata": {
        "id": "certain-wrestling",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f5122bf6-da85-4fcc-d528-87d13b50793c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(x**2 - x + 1)*exp(y**2) + (y**2 - y + 1)*exp(x**2)"
            ],
            "text/latex": "$\\displaystyle \\left(x^{2} - x + 1\\right) e^{y^{2}} + \\left(y^{2} - y + 1\\right) e^{x^{2}}$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Minimum value at x = [0.2778799 0.2778799]\n",
            "Function value at minimum: 1.7270110514248864\n",
            "Gradient norm at minimum: 9.643289486085021e-07\n",
            "Number of iterations: 387\n"
          ]
        }
      ],
      "source": [
        "# Define variables and functions\n",
        "x, y = sp.symbols('x y')\n",
        "g = sp.Function('g', real=True)\n",
        "g = (1 - x + x**2) * sp.exp(y**2) + (1 - y + y**2) * sp.exp(x**2)\n",
        "display(g)\n",
        "\n",
        "g_x = g.diff(x)\n",
        "g_y = g.diff(y)\n",
        "\n",
        "g = sp.lambdify([x,y], g)\n",
        "g_x = sp.lambdify([x,y], g_x)\n",
        "g_y = sp.lambdify([x,y], g_y)\n",
        "\n",
        "# Define the numpy functions\n",
        "def g_func(x):\n",
        "    return g(x[0], x[1])\n",
        "\n",
        "def grad_g(x):\n",
        "    return np.array([g_x(x[0], x[1]), g_y(x[0], x[1])])\n",
        "\n",
        "# Running the Gradient Descent Method\n",
        "def gradient_descent(f, grad_f, x0, alpha = 0.01, max_iter = 1000, tol = 1e-6):\n",
        "    xk = x0\n",
        "    k = 0\n",
        "\n",
        "    while k < max_iter and np.linalg.norm(grad_f(xk)) > tol:\n",
        "        k += 1\n",
        "        xk = xk - alpha * grad_f(xk)\n",
        "\n",
        "    return (xk, f(xk), np.linalg.norm(grad_f(xk)), k)\n",
        "\n",
        "x0 = np.array([0.0, 0.0])\n",
        "print()\n",
        "xk, min_value, grad_norm, iterations = gradient_descent(g_func, grad_g, x0, alpha=0.01)\n",
        "\n",
        "print(f\"Minimum value at x = {xk}\")\n",
        "print(f\"Function value at minimum: {min_value}\")\n",
        "print(f\"Gradient norm at minimum: {grad_norm}\")\n",
        "print(f\"Number of iterations: {iterations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "functioning-event",
      "metadata": {
        "id": "functioning-event"
      },
      "source": [
        "## Problem 3\n",
        "On a certain workday, the *rate*, in tons per hour, at which unprocessed gravel arrives at a gravel processing plant is modeled by $G(t) = 90 + 45\\cdot \\cos‚Å°\\left(\\frac{t^2}{18}\\right)$, where $t$ is measured in hours and $0 \\leqslant t \\leqslant 8$. At the beginning of the workday, $t = 0$, the plant has 500 tons of unprocessed gravel. During the hours of operation, $0 \\leqslant t \\leqslant 8$, the plant processes gravel at a constant rate $P(t) = 100$ tons per hour.\n",
        "* Find the total amount of unprocessed gravel that arrives at the plant during the hours of operation on this workday.\n",
        "* Is the amount of unprocessed gravel at the end of the workday (t=8) greater or smaller than amount of gravel at the beginning of the workday?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solid-choice",
      "metadata": {
        "id": "solid-choice"
      },
      "outputs": [],
      "source": [
        "# Define variable and functions\n",
        "\n",
        "\n",
        "# First bullet point\n",
        "\n",
        "\n",
        "# Second bullet point\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "destroyed-start",
      "metadata": {
        "id": "destroyed-start"
      },
      "source": [
        "## Problem 4\n",
        "Solve the system of equations:\n",
        "\\begin{equation}\n",
        "\\left\\{\n",
        "\\begin{array}{rcl}\n",
        "x - 2y + 3z &=& -1\\\\\n",
        "3x + 2y - 5z &=& 3\\\\\n",
        "2x - 5y + 2z &=& 0\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "using **Gradient Descent Method** applied to a function of the kind $f(x) = \\|A\\mathbf{x} - b\\|_2^2$ where $A\\mathbf{x} = b$ is the matrix equation that corresponds to the system, and $\\| \\cdot \\|_2$ is the Euclidean norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fallen-hours",
      "metadata": {
        "id": "fallen-hours",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea0f9ad-8bd4-4505-f606-eb63e817913d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.26086976, -0.08695635, -0.4782607 ]),\n",
              " 1.530250529911669e-13,\n",
              " 9.830335419305582e-07,\n",
              " 509)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Define variables and matrices\n",
        "x,y,z = sp.symbols('x y z', real = True)\n",
        "\n",
        "# Define the function and derivatives\n",
        "f = (x-2*y+3*z+1)**2 + (3*x+2*y-5*z-3)**2 + (2*x-5*y+2*z)**2\n",
        "\n",
        "f_x = f.diff(x)\n",
        "f_y = f.diff(y)\n",
        "f_z = f.diff(z)\n",
        "\n",
        "f = sp.lambdify([x,y,z], f)\n",
        "f_x = sp.lambdify([x,y,z], f_x)\n",
        "f_y = sp.lambdify([x,y,z], f_y)\n",
        "f_z = sp.lambdify([x,y,z], f_z)\n",
        "\n",
        "# Define the NumPy function\n",
        "def f_func(x):\n",
        "    return f(x[0], x[1], x[2])\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([f_x(x[0], x[1], x[2]), f_y(x[0], x[1], x[2]), f_z(x[0], x[1], x[2])])\n",
        "\n",
        "# Running the Gradient Descent Method\n",
        "xk = np.array([6, -1, 1])\n",
        "gradient_descent(f_func, grad_f, xk, alpha = 0.01, max_iter=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pozFzUeQB1f",
      "metadata": {
        "id": "7pozFzUeQB1f"
      },
      "source": [
        "## Problem 5\n",
        "\n",
        "[Use Gradient Descent Method] Four people are typing two paragraphs of text. The first paragraph uses elementary vocabulary, while the second paragraph contains some more advanced words. The number of typos that the people make are labeled by ùë•, for the first paragraph, and ùë¶, for the second paragraph. Data collected is given in the accompanying table:\n",
        "\n",
        "| x \t | y \t|\n",
        "|---\t |---\t|\n",
        "| 1 \t | 4 \t|\n",
        "| 3 \t | 5 \t|\n",
        "| 4 \t | 6 \t|\n",
        "| 5 \t | 8 \t|\n",
        "\n",
        "\n",
        "We will build a model to predict the number of typos $y$ in the second text based on the number of typos in the first test. To do this, we use least-squares regression, an approach similar to the one we used for systems. For every number of observed typos $x_i$ for the first text, we have a corresponding number $y_i$ of observed typos for the second text. Our model $\\hat{y_i} = f(x_i)$ will make a prediction of the number of typos on the second text. Most often, the observed number of typos $y_i$ and the predicted number of typos $\\hat{y_i}$ will not be equal.\n",
        "Let us label each such discrepancy in values by $d_i$, in other words:\n",
        "\n",
        "\\begin{align}\n",
        "d_i = observed_i - predicted_i = y_i - \\hat{y_i}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "We call these discrepancies residuals. The goal now is to choose a model which will minimize the total sum of the squares of the residuals, i.e. a model which will make the overall discrepancy between the observed data and the predictions based on it as small as possible. Note that the form of the residuals depends on the choice of the model. For example, choosing a linear model $\\hat{y_i}=a+bx$, we have prediction $\\hat{y_i}=a+bx$, so the residuals become:\n",
        "\n",
        "\\begin{align}\n",
        "d^2 = (y-\\hat{y})^2 = (y-a-bx)^2\n",
        "\\end{align}\n",
        "\n",
        "The total sum of the squares of the residuals is then $\\sum_{i=1}^n d_i^2 = \\sum_{i=1}^n (y_i-a-bx_i)^2$. This allows us to define the function $f$ that we need to minimize in order to build the model as:\n",
        "\n",
        "\\begin{align}\n",
        "f(a,b) = \\sum_{i=1}^n(y_i-a-bx_i)^2\n",
        "\\end{align}\n",
        "\n",
        "By minimizing this function, we find the values for the coefficients $a$ and $b$ in the model that minimize the discrepancy between the data and the type of model we chose to work with.\n",
        "\n",
        "*a)* Build a linear model for the data: $\\hat{y}=a+bx$. Then make a prediction about the number of typos $y$ a person would make when typing the second text if they have made $x=2$ typos when typing the first text. **Use your own gradient_descent function.**\n",
        "\n",
        "*b)* Build a quadratic model for the data: $\\hat{y}=a+bx+cx^2$. Then make a prediction about the same person from part a). **Use the minimizer BFGS.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qCNOYmmxUjzO",
      "metadata": {
        "id": "qCNOYmmxUjzO"
      },
      "source": [
        "**a) Linear model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tDW7nTRgP-JD",
      "metadata": {
        "id": "tDW7nTRgP-JD"
      },
      "outputs": [],
      "source": [
        "# Define variables and matrices\n",
        "x = np.array([1, 3, 4, 5])\n",
        "y = np.array([4, 5, 6, 8])\n",
        "\n",
        "# Define the function f\n",
        "def f(x, y, a, b):\n",
        "    return np.sum((y - a - b * x) ** 2)\n",
        "\n",
        "# Define the numpy function f\n",
        "def f_func_linear(x0, x, y):\n",
        "    a, b = x0\n",
        "    return f(x, y, a, b)\n",
        "\n",
        "def grad_f_linear(x0, x, y):\n",
        "    a, b = x0\n",
        "    grad_a = -2 * np.sum(y - a - b * x)\n",
        "    grad_b = -2 * np.sum((y - a - b * x) * x)\n",
        "    return np.array([grad_a, grad_b])\n",
        "\n",
        "def gradient_descent(f, grad_f, x0, alpha = 0.01, max_iter = 1000, tol = 1e-6):\n",
        "    xk = x0\n",
        "    k = 0\n",
        "\n",
        "    while k < max_iter and np.linalg.norm(grad_f(xk, x, y)) > tol:\n",
        "        k += 1\n",
        "        xk = xk - alpha * grad_f(xk, x, y)\n",
        "\n",
        "    return (xk, f(xk, x, y), np.linalg.norm(grad_f(xk, x, y)), k)\n",
        "\n",
        "x0 = np.array([0.0, 0.0])\n",
        "\n",
        "optimal_params_linear, final_value, grad_norm, iterations = gradient_descent(f_func_linear, grad_f_linear, x0)\n",
        "a_1, b_2 = optimal_params_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x4OY0J9zVPbP",
      "metadata": {
        "id": "x4OY0J9zVPbP"
      },
      "outputs": [],
      "source": [
        "solution_vector = np.array([a_1, b_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rd1d-4WqV2pA",
      "metadata": {
        "id": "rd1d-4WqV2pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626cedec-2587-4584-86ef-4ea65c1ae258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear model solution vector: [2.68570891 0.94285853]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Linear model solution vector: {solution_vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction for x = 2\n",
        "x_test = 2\n",
        "y_pred_linear = a_1 + b_2 * x_test\n",
        "print(f\"Predicted y for x = 2: {y_pred_linear}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW02CaEklluU",
        "outputId": "e563645e-1666-4a3f-a4bd-77baf6f6c8e3"
      },
      "id": "kW02CaEklluU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted y for x = 2: 4.571425970381102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YzzxuAtIUsC6",
      "metadata": {
        "id": "YzzxuAtIUsC6"
      },
      "source": [
        "**b) Quadratic model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E-7Kct6HUikW",
      "metadata": {
        "id": "E-7Kct6HUikW"
      },
      "outputs": [],
      "source": [
        "# Define the function f\n",
        "def f(x, y, a, b, c):\n",
        "    return np.sum((y - (a + b * x + c * x**2)) ** 2)\n",
        "\n",
        "# Define the numpy function f\n",
        "def f_func_quadratic(x0, x, y):\n",
        "    a, b, c = x0\n",
        "    return f(x, y, a, b, c)\n",
        "\n",
        "def grad_f_quadratic(x0, x, y):\n",
        "    a, b, c = x0\n",
        "    grad_a = -2 * np.sum(y - a - b * x - c * x**2)\n",
        "    grad_b = -2 * np.sum((y - a - b * x - c * x**2) * x)\n",
        "    grad_c = -2 * np.sum((y - a - b * x - c * x**2) * x**2)\n",
        "    return np.array([grad_a, grad_b, grad_c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2-_fEnpgUxaS",
      "metadata": {
        "id": "2-_fEnpgUxaS"
      },
      "outputs": [],
      "source": [
        "# Run the BFGS algorithm\n",
        "x0_quadratic = np.array([0.0, 0.0, 0.0])\n",
        "result = opt.minimize(f_func_quadratic, x0_quadratic, args=(x, y), jac=grad_f_quadratic, method='BFGS')\n",
        "a_opt, b_opt, c_opt = result.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLWXNrVpV-xg",
      "metadata": {
        "id": "KLWXNrVpV-xg"
      },
      "outputs": [],
      "source": [
        "solution_vector = np.array([a_opt, b_opt, c_opt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ncOkIOkEWyGF",
      "metadata": {
        "id": "ncOkIOkEWyGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be052b3-7dca-4e17-cd2a-13929551503b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quadratic model solution vector: [ 4.4        -0.65454545  0.27272727]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Quadratic model solution vector: {solution_vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = 2\n",
        "y_pred_quadratic = a_opt + b_opt * x_test + c_opt * x_test**2\n",
        "print(f\"Predicted y for x = 2: {y_pred_quadratic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvj9hTUTp7kP",
        "outputId": "c2c04603-cf21-45e0-acf6-418d68915375"
      },
      "id": "Bvj9hTUTp7kP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted y for x = 2: 4.181818181818182\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}